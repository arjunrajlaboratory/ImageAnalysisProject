# SAM2 Video

This worker uses SAM2's **video predictor** to track objects across time or Z-slices. Unlike `sam2_propagate` which processes frames independently, this worker uses SAM2's native video prediction pipeline (`build_sam2_video_predictor` / `propagate_in_video`) which maintains temporal context across all frames simultaneously for more consistent tracking.

## How It Works

1. **Annotation Retrieval**: Fetches all polygon annotations matching the specified tracking tag
2. **Video Sequence Setup**: For each (XY, Z) or (XY, Time) combination, constructs a "video" sequence from the batch frames along the tracking dimension
3. **Prompt Initialization**: Adds each source annotation as a bounding box prompt at its corresponding frame index in the video sequence, with a unique object ID
4. **Video Propagation**: Runs `predictor.propagate_in_video()` which processes all frames jointly, propagating masks with temporal/spatial context
5. **Mask Conversion**: Thresholds output logits at 0.0, converts masks to polygons with padding and smoothing
6. **Connection Creation**: Creates sequential parent-child connections between annotations of the same object across frames, tagged with `SAM2_VIDEO`

## Interface Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| **Batch XY** | text | Current tile | XY positions to process (e.g., "1-3, 5-8") |
| **Batch Z** | text | Current tile | Z positions to process |
| **Batch Time** | text | Current tile | Time positions to process |
| **Track across** | select | Time | Dimension to track along: "Time" or "Z" |
| **Track direction** | select | Forward | Direction of tracking: "Forward" or "Backward" |
| **Model** | select | `sam2.1_hiera_small.pt` | SAM2.1 model checkpoint |
| **Tag of objects to track** | tags | -- | Tag(s) identifying source annotations to track |
| **Connect sequentially** | checkbox | True | Create parent-child connections between frames |
| **Padding** | number | 0 | Expand (+) or shrink (-) polygons in pixels (-20 to 20) |
| **Smoothing** | number | 0.3 | Polygon simplification tolerance (0 to 3) |

## Implementation Details

### Video Predictor vs. Image Predictor

This is the key distinction from `sam2_propagate`. The video predictor (`build_sam2_video_predictor`) processes all frames in a sequence together, using SAM2's memory attention mechanism to maintain consistent object representations across frames. The image predictor (used by `sam2_propagate`) treats each frame independently using only bounding box prompts from the previous frame.

The video predictor is expected to produce more temporally consistent tracking, especially for objects that undergo significant appearance changes, because it jointly reasons about all frames rather than chaining single-frame predictions.

### Video Data Format

The worker constructs a video data dictionary containing the `tileClient`, `datasetId`, and a list of (XY, Z, Time) batch tuples. This is passed to `predictor.init_state(video_path=video_data)` which loads the image sequence. Each frame in the video corresponds to one step along the tracking dimension (Time or Z).

### Object Identity Tracking

Each source annotation is assigned a sequential integer `obj_id` (0, 1, 2, ...) when added as a prompt. This ID is preserved through propagation so that the output masks at each frame can be associated back to their source object. After upload, connections are generated by grouping annotations by `obj_id` and sorting by Time/Z to create sequential parent-child chains.

### Tracking Direction

When "Backward" is selected, the batch sequence along the tracking dimension is reversed before constructing the video. This means the video predictor sees frames in reverse order and propagates from later frames to earlier ones.

### Connection Generation

Connections are always created with the earlier frame as parent and later frame as child, regardless of propagation direction. Annotations are sorted by (Time, Z) within each object ID group, and consecutive pairs are connected with the tag `SAM2_VIDEO`.

### Model and Checkpoint Path

Uses checkpoints from `/code/segment-anything-2-nimbus/checkpoints/` (note: different path than the other SAM2 workers which use `/code/sam2/checkpoints/`). Same SAM2.1 Hiera model variants are available: tiny, small (default), base_plus, and large.

### GPU Handling

Requires CUDA GPU. Enables bfloat16 autocast and TF32 on Ampere GPUs.

## Notes

- This worker uses `propagate_in_video()` which processes the entire frame sequence at once. For very long sequences, this may require significant GPU memory. Consider using `sam2_propagate` for memory-constrained situations.
- Connections are always created (using the `SAM2_VIDEO` tag) when `Connect sequentially` is enabled. The `connect_sequentially` parameter from the interface controls this.
- Multiple annotations at different frames can be provided as prompts simultaneously -- each gets its own `obj_id` and is tracked independently through the video.
- Related workers: `sam2_propagate` (frame-by-frame propagation using image predictor), `sam2_automatic_mask_generator` (automatic detection without tracking).
